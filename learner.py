from config import StochasticMuZeroConfig
from utils import ReplayBuffer, Network, NetworkCacher

class Learner(metaclass=abc.ABCMeta):
    """An learner to update the network weights based.
    """
    @abc.abstractmethod
    def learn(self):
        """Single training step of the learner.
        """
    @abc.abstractmethod
    def export(self) -> Network:
        """Exports the network.
        """

def policy_loss(predictions, labels):
    """Minimizes the KL-divergence of the predictions and labels."""
    return 0.0

def compute_td_target(td_steps, td_lambda, trajectory):
    """Computes the TD lambda targets given a trajectory for the 0th
    element.
    Args:
    td_steps: The number n of the n-step returns.
    td_lambda: The lambda in TD(lambda).
    trajectory: A sequence of states.
    Returns:
    The n-step return.
    """
    return 0.0

def value_or_reward_loss(prediction, target):
    """Implements the value or reward loss for Stochastic MuZero.
    For backgammon this is implemented as an MSE loss of scalars.
    For 2048, we use the two hot representation proposed in
    MuZero, and this loss is implemented as a KL divergence between the
    value
    and value target representations.
    For 2048 we also apply a hyperbolic transformation to the target (see
    paper
    for more information).
    Args:
    prediction: The reward or value output of the network.
    target: The reward or value target.
    Returns:
    The loss to minimize."""
    return 0.0


class StochasticMuZeroLearner(Learner):
    """Implements the learning for Stochastic MuZero.
    """
    def __init__(self,
        config: StochasticMuZeroConfig,
        replay_buffer: ReplayBuffer):
        self.config = config
        self.replay_buffer = replay_buffer
        # Instantiate the network.
        self.network = config.network_factory()
    def transpose_to_time(self, batch):
        """Transposes the data so the leading dimension is time instead of
        batch.
        """
        return batch
    def learn(self):
        """Applies a single training step.
        batch = self.replay_buffer.sample()
        """
        # Transpose batch to make time the leading dimension.
        batch = self.transpose_to_time(batch)
        # Compute the initial step loss.
        latent_state = self.network.representation(batch[0].observation)
        predictions = self.network.predictions(latent_state)
        # Computes the td target for the 0th position.
        value_target = compute_td_target(self.config.td_steps,
        self.config.td_lambda,
        batch)
        # Train the network value towards the td target.
        total_loss = value_or_reward_loss(predictions.value, value_target)
        # Train the network policy towards the MCTS policy.
        total_loss += policy_loss(predictions.probabilities,
        batch[0].search_stats.search_policy)
        # Unroll the model for k steps.
        for t in range(1, self.config.num_unroll_steps + 1):
            # Condition the afterstate on the previous action.
            afterstate = self.network.afterstate_dynamics(
            latent_state, batch[t - 1].action)
            afterstate_predictions = self.network.afterstate_predictions(
            afterstate)
            # Call the encoder on the next observation.
            # The encoder returns the chance code which is a discrete one hot code.
            # The gradients flow to the encoder using a straight through estimator.
            chance_code = self.network.encoder(batch[t].observation)
            # The afterstate value is trained towards the previous value target
            # but conditioned on the selected action to obtain a Q-estimate.
            total_loss += value_or_reward_loss(
            afterstate_predictions.value, value_target)

            # The afterstate distribution is trained to predict the chance code
            # generated by the encoder.

            total_loss += policy_loss(afterstate_predictions.probabilities,
            chance_code)
            # Get the dynamic predictions.
            latent_state = self.network.dynamics(afterstate, chance_code)
            predictions = self.network.predictions(latent_state)
            # Compute the new value target.
            value_target = compute_td_target(self.config.td_steps,
            self.config.td_lambda,
            batch[t:])
            # The reward loss for the dynamics network.
            total_loss += value_or_reward_loss(predictions.reward, batch[t].
            reward)
            total_loss += value_or_reward_loss(predictions.value, value_target)
            total_loss += policy_loss(predictions.probabilities,
            batch[t].search_stats.search_policy)
        minimize_with_adam_and_weight_decay(total_loss,
                learning_rate=self.config.
                learning_rate,
                weight_decay=self.config.
                weight_decay)
    
    def export(self) -> Network:
        return self.network
    
def train_stochastic_muzero(config: StochasticMuZeroConfig,
    cacher: NetworkCacher,
    replay_buffer: ReplayBuffer):
    learner = StochasticMuZeroLearner(config, replay_buffer)
    # Export the network so the actors can start generating experience.
    cacher.save_network(0, learner.export())
    for step in range(config.training_steps):
        # Single learning step.
        learner.learn()
        if step > 0 and step % config.export_network_every == 0:
            cacher.save_network(step, learner.export())