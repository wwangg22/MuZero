from typing import Optional, List, Sequence, Tuple, Union, Any
from config import *
from typesMZ import Trajectory
MAXIMUM_FLOAT_VALUE = float('inf')

Player = int

Action= Any

Outcome = Any

ActionOrOutcome = Union[Action, Outcome]

class Node(object):
    """A Node in the MCTS search tree.
    """
    def __init__(self,
    prior: float,
    is_chance: bool = False):
        self.visit_count = 0
        self.to_play = -1
        self.prior = prior
        self.value_sum = 0
        self.children = {}
        self.state = None
        self.is_chance = is_chance
        self.reward = 0
    def expanded(self) -> bool:
        return len(self.children) > 0
    def value(self) -> float:
        if self.visit_count == 0:
            return 0
        return self.value_sum / self.visit_count
    
class ActionOutcomeHistory:
    """Simple history container used inside the search.
    Only used to keep track of the actions and chance outcomes executed.
    """
    def __init__(self,
        player: Player,
        history: Optional[List[ActionOrOutcome]] = None):
        self.initial_player = player
        self.history = list(history or [])
    def clone(self):
        return ActionOutcomeHistory(self.initial_player, self.history)
    def add_action_or_outcome(self, action_or_outcome: ActionOrOutcome):
        self.history.append(action_or_outcome)
    def last_action_or_outcome(self) -> ActionOrOutcome:
        return self.history[-1]
    def to_play(self) -> Player:
        # Returns the next player to play based on the initial player and the
        # history of actions and outcomes. For example for backgammon the two
        # players alternate, while for 2048 it is always the same player.
        return 0
    
class ReplayBuffer:
    """A replay buffer to hold the experience generated by the selfplay.
    """
    def __init__(self, config: StochasticMuZeroConfig):
        self.config = config
        self.data = []
    def save(self, seq: Trajectory):
        if len(self.data) > self.config.num_trajectories_in_buffer:
        # Remove the oldest sequence from the buffer.
            self.data.pop(0)
        self.data.append(seq)
    def sample_trajectory(self) -> Trajectory:
        """Samples a trajectory uniformly or using prioritization.
        return self.data[0]
        """
    def sample_index(self, seq: Trajectory) -> int:
        """Samples an index in the trajectory uniformly or using
        prioritization.
        """
        return 0
    def sample_element(self) -> Trajectory:
        """Samples a single element from the buffer.
        """
        # Sample a trajectory.
        trajectory = self.sample_trajectory()
        state_idx = self.sample_index(trajectory)
        limit = max([self.config.num_unroll_steps, self.config.td_steps])
        # Returns a trajectory of experiment.
        return trajectory[state_idx:state_idx + limit]
    def sample(self) -> Sequence[Trajectory]:
        """Samples a training batch.
        """
        return [self.sample_element() for _ in range(self.config.batch_size)]
    
class MinMaxStats(object):
    """A class that holds the min-max values of the tree.
    """
    def __init__(self, known_bounds: Optional[KnownBounds]):
        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE
        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE
    def update(self, value: float):
        self.maximum = max(self.maximum, value)
        self.minimum = min(self.minimum, value)
    def normalize(self, value: float) -> float:
        if self.maximum > self.minimum:
            # We normalize only when we have set the maximum and minimum values
            return (value - self.minimum) / (self.maximum - self.minimum)
        return value

class NetworkCacher:
    """An object to share the network between the self-play and training
    jobs.
    """
    def __init__(self):
        self._networks = {}
    def save_network(self, step: int, network: Network):
        self._networks[step] = network
    def load_network(self) -> Tuple[int, Network]:
        training_step = max(self._networks.keys())
        return training_step, self._networks[training_step]


class Environment:
    """Implements the rules of the environment.
    """
    def apply(self, action: Action):
        """Applies an action or a chance outcome to the environment.
        """
    def observation(self):
        """Returns the observation of the environment to feed to the network.
        """
    def is_terminal(self) -> bool:
        """Returns true if the environment is in a terminal state.
        return False
        """
    def legal_actions(self) -> Sequence[Action]:
        """Returns the legal actions for the current state.
        return []
        """
    def reward(self, player: Player) -> float:
        """Returns the last reward for the player.
        return 0.0
        """
    def to_play(self) -> Player:
        """Returns the current player to play.
        return 0"""